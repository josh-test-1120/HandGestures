{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Machine Learning | Hand Motion Recognition Project</title>
  <link rel="stylesheet" href="{% static 'main_app/css/style.css' %}">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet">
  <link href="https://unpkg.com/aos@2.3.4/dist/aos.css" rel="stylesheet">
</head>

<body class="bg-light text-dark">
  {% include 'main_app/navbar.html' %}

  <main class="container wide-container py-5 mb-5">
    <!-- Hero Section -->
    <section class="mb-5">
      <div class="text-center text-white p-5 rounded-4 mx-3" style="background: linear-gradient(to right, #31b3fe, #7d7676);" data-aos="fade-down">
        <h1 class="fw-bold display-5 mb-4">Machine Learning Model</h1>
        <h2 class="lead text-light mb-0">Explore the algorithms, data processing, and training techniques behind our real-time motion recognition system.</h2>
      </div>
    </section>

    <!-- ML Flow Prototype Section -->
<section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
  <h2 class="fw-bold text-primary mb-4">ML Flow Prototype</h2>
  <figure class="text-center">
    <img src="{% static 'main_app/img/ml-prototype.png' %}" 
         alt="ML Flow Prototype Diagram" 
         class="img-fluid shadow rounded mx-auto mb-2" 
         style="max-width: 1000px;">
    <figcaption class="text-muted fw-semibold small">
      Citation: <a href="https://doi.org/10.1007/s11227-023-05299-9" target="_blank">Flow Prototype</a>
    </figcaption>
  </figure>
  <p class="text-muted fs-5" style="text-align: justify;">
    This flow diagram illustrates the complete machine learning pipeline used in our seizure detection system. From raw data acquisition to classification, each stage is designed to ensure reliable real-time performance in distinguishing between seizure and non-seizure motion patterns.
  </p>

  <p class="text-muted fs-5" style="text-align: justify;">
    1. <strong>Data Collection:</strong> IMU sensors capture real-time acceleration and rotation data from the user.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    2. <strong>Preprocessing:</strong> Raw data is cleaned, normalized, and segmented into time windows for analysis.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    3. <strong>Feature Extraction:</strong> Statistical and frequency-based features are extracted to represent motion patterns.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    4. <strong>Deep Learning Models:</strong> Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are trained on labeled sensor data to learn complex temporal and spatial motion patterns.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    5. <strong>Classification:</strong> The system classifies each time window into one of four categories: <em>Normal</em>, <em>Tremor</em>, <em>Tonic</em>, or <em>Postural</em>. This multi-class approach enables a more detailed understanding of the user's motion state and supports early intervention.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    6. <strong>Performance Evaluation:</strong> Accuracy, precision, recall, and confusion matrix are used to assess system effectiveness.
  </p>
</section>

<!-- Data Collection section-->
<section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
  <h2 class="fw-bold text-primary mb-4">Data Collection</h2>
  <div class="row mb-3">
    <div class="col-md-6">
      <img src="{% static 'main_app/img/hardware3.jpeg' %}" alt="Internal Hardware View" class="img-fluid shadow rounded mx-auto d-block" style="max-height: 504px;">
    </div>
    <div class="col-md-6">
      <img src="{% static 'main_app/img/hardware_outside.jpg' %}" alt="External Interface View" class="img-fluid shadow rounded mx-auto d-block" style="max-height: 504px;">
    </div>
  </div>
  <p class="text-muted fs-5" style="text-align: justify;">
    The wearable device is housed in a custom-built cardboard enclosure and includes a set of sensors and electronics for motion and distance tracking. 
    On the outside, a tactile push-button allows user interaction, and a compact OLED screen is embedded to display live feedback or status updates.
    Inside the enclosure, the system integrates multiple components wired together, including:
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    1. <strong>Microcontroller Unit:</strong> A central microcontroller (such as Arduino Nano) handles data processing and sensor communication.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    2. <strong>IMU Module:</strong> Includes a gyroscope and accelerometer to track x, y, and z rotational and acceleration movements.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    3. <strong>Ultrasonic Distance Sensors:</strong> Positioned to the sides to detect left and right distance from nearby surfaces for spatial awareness.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    4. <strong>OLED Display:</strong> Mounted externally, it provides real-time display of sensor readings or status info.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    5. <strong>Push Button:</strong> A user interface component to trigger data collection or mode changes.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    6. <strong>Power Supply:</strong> A small Li-Po battery or USB connection powers the internal components.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    All collected data—such as motion vectors and proximity readings—is logged and written to a CSV file on the connected computer. 
    This setup is crucial for later analysis, such as training machine learning models for motion classification or seizure detection.
  </p>
</section>


    <!-- Data Pipeline Explanation Section -->
<section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
  <h2 class="fw-bold text-primary mb-4 text-center">Data Processing and Classification Pipeline</h2>
  
  <img src="{% static 'main_app/img/preprocessing.png' %}" alt="Preprocessing and Feature Extraction" class="img-fluid shadow rounded mx-auto d-block mb-4" style="max-width: 1000px;">

  <div class="text-muted fs-5" style="text-align: justify;">
    <p class="fs-5"><strong>Raw Sensor Data:</strong> The system captures data from wearable sensors, including:</p>
    <p class="fs-5">1. <strong>Accelerometer (X, Y, Z):</strong> Detects movement intensity and direction.</p>
    <p class="fs-5">2. <strong>Gyroscope (Roll, Pitch, Yaw):</strong> Tracks orientation and rotational motion.</p>
    <p class="fs-5">3. <strong>Ultrasound (Distance):</strong> Measures proximity to surfaces, supporting fall detection analysis.</p>

    <p class="fs-5"><strong>Preprocessing:</strong> Before analysis, the raw sensor data undergoes several cleaning and preparation steps:</p>
    <p class="fs-5">4. <strong>Filtering:</strong> Removes signal noise to produce cleaner, more reliable data.</p>
    <p class="fs-5">5. <strong>Segmentation:</strong> Splits data into consistent 5-second windows for time-based analysis.</p>
    <p class="fs-5">6. <strong>Normalization:</strong> Aligns value ranges across features for uniform input into the model.</p>

    <p class="fs-5"><strong>Feature Extraction:</strong> Key features are derived from sensor readings using three analytical domains:</p>
    <p class="fs-5">7. <strong>Time-Domain:</strong> Includes jerk (dAccel/dt), zero-crossing rate, and signal magnitude area.</p>
    <p class="fs-5">8. <strong>Frequency-Domain:</strong> Extracts power spectral density (PSD) peaks at 3–5Hz and spectral entropy values.</p>
    <p class="fs-5">9. <strong>Time-Frequency Domain:</strong> Utilizes wavelet coefficients to capture dynamic motion patterns over time and frequency.</p>

    <p class="fs-5"><strong>Model Inference:</strong> An LSTM Neural Network (with three hidden layers and dropout regularization) processes the extracted features. A confidence threshold ensures only predictions above 90% certainty are accepted.</p>

    <p class="fs-5"><strong>Final Output:</strong> The model classifies the user’s movement into the following categories:</p>
    <p class="fs-5">10. <strong>Normal Movement</strong></p>
    <p class="fs-5">11. <strong>Tonic-Clonic Seizure:</strong> Characterized by rhythmic motion patterns around 3–5Hz frequency.</p>
    <p class="fs-5">12. <strong>Myoclonic Seizure:</strong> Identified by sudden, shock-like jerking movements.</p>
    <p class="fs-5">13. <strong>Postural Change / Fall:</strong> Detected using wavelet pattern recognition for abrupt position shifts.</p>
  </div>
</section>


<!--Machine Learning Methods-->
<section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
  <h2 class="fw-bold text-primary mb-4">Methods</h2>

  <p class="text-muted fs-5" style="text-align: justify;">
    Our system leverages a <strong>Semi-Supervised Learning</strong> approach to overcome the challenge of limited labeled sensor data. Initially, a small set of motion samples (such as Normal, Tonic-Clonic, Myoclonic, and Fall) were manually annotated. These samples were used to train a baseline model.
  </p>

  <p class="text-muted fs-5" style="text-align: justify;">
    This model was then used to assign <strong>pseudo-labels</strong> to a larger pool of unlabeled motion data. After selective verification, this expanded dataset improved model robustness while reducing manual labeling effort.
  </p>

  <p class="text-muted fs-5" style="text-align: justify;">
    The core architecture used in our project is a <strong>Long Short-Term Memory (LSTM)</strong> neural network, specifically designed to recognize patterns in time-series motion data.
  </p>

  <p class="text-muted fs-5" style="text-align: justify;">
    1. <strong>Semi-Supervised Labeling:</strong> Combines a small amount of labeled data with a large pool of unlabeled data using a pseudo-labeling strategy.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    2. <strong>LSTM Network:</strong> Utilizes a 3-layer recurrent architecture with dropout regularization to detect temporal patterns in motion sequences.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    3. <strong>Sliding Window Input:</strong> Segments incoming motion data into 5-second windows, allowing the model to analyze sequential trends.
  </p>
  <p class="text-muted fs-5" style="text-align: justify;">
    4. <strong>Confidence Filtering:</strong> Only model predictions with over 90% confidence are accepted for real-time alerts or used to reinforce training.
  </p>

  <p class="text-muted fs-5" style="text-align: justify;">
    This method balances labeling efficiency and model accuracy, enabling real-time classification of complex motion patterns such as seizure events and posture-related incidents.
  </p>
</section>


    <!-- Classification Section -->
    <section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
      <h2 class="fw-bold text-primary mb-4">Classification</h2>
      <img src="{% static 'main_app/img/classifications.png' %}" alt="Classifications" class="img-fluid shadow rounded mx-auto d-block mb-3" style="max-width: 1000px;">
    
      <p class="text-muted fs-5" style="text-align: justify;">
        Our seizure detection system classifies motion into four categories based on sensor input. These categories help distinguish between everyday movement and potentially dangerous seizure activity.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        1. <strong>Normal (No Seizure):</strong>  
        This classification represents typical daily movements such as walking, writing, adjusting posture, and other normal everyday activities.. It indicates a stable condition with no irregular muscle contractions, no involuntary jerking, and consistent orientation. Sensor readings show smooth acceleration and rotation patterns within normal thresholds.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        2. <strong>Tremor (Hand Seizure):</strong>  
        Characterized by rhythmic shaking or trembling, usually in the hands. This type of seizure often results in rapid, repetitive motion patterns detected in the accelerometer data. The system identifies these patterns as fine oscillations with high frequency but low amplitude.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        3. <strong>Tonic (Arm Stiffening Seizure):</strong>  
        Marked by sudden and sustained muscle contractions, typically causing the arms to become rigid. The sensors detect a sudden increase in resistance to motion, minimal limb movement, and consistent stiff posturing. These are recognized through near-zero dynamic acceleration despite continuous muscle tension.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        4. <strong>Postural (Posture Change Seizure):</strong>  
        Involves sudden changes in body position or loss of balance, often leading to falls or abrupt leaning. The system identifies this through rapid shifts in acceleration and orientation, especially when combined with a drop in height or sideways tilting, suggesting instability or collapse.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        These classifications allow the system to provide real-time alerts, support caregivers, and improve safety for individuals with epilepsy by monitoring movement patterns continuously and non-invasively.
      </p>
    </section>

    <!-- Performance Evaluation Section 
    <section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
      <h2 class="fw-bold text-primary mb-4">Performance Evaluation</h2>
      <img src="{% static 'main_app/img/prediction_accuracy.png' %}" alt="Prediction Accuracy" class="img-fluid shadow rounded mx-auto d-block mb-3" style="max-width: 1000px;">
    
      <p class="text-muted fs-5" style="text-align: justify;">
        Our trained model achieved a high prediction accuracy of <strong>98.76%</strong>, calculated by comparing predicted labels with ground-truth annotations.
        This metric reflects the percentage of correct classifications across all categories.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        In addition to accuracy, further evaluation metrics such as <strong>precision, recall,</strong> and <strong>F1-score</strong> are calculated for each class 
        (Normal, Tremor, Tonic, Postural) to assess classification performance more comprehensively.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        Evaluation is based on a labeled dataset of <em>10000</em> samples, using a <em>87.65%</em> train/test split. 
        The model architecture is based on <em>4</em> types, optimized for real-time classification with wearable motion data.
      </p>
    </section>
  -->
    
    <!--
    TODO: decide if we want to include this section (commented out on purpose as of now; 5-12-2025)
    ML Pipeline Diagram Section 

    <section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
      <h2 class="fw-bold text-primary mb-4">ML Pipeline Diagram</h2>
      <img src="{% static 'main_app/img/ml_pipeline.png' %}" alt="Machine Learning Pipeline Diagram" class="img-fluid mx-auto d-block mb-3" style="max-width: 900px;">

    </section>
        
    -->
    
  </main>

  {% include 'main_app/footer.html' %}

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
  <script src="https://unpkg.com/aos@2.3.4/dist/aos.js"></script>
  <script>
    AOS.init({ duration: 1000, once: true });
  </script>
  <script src="{% static 'main_app/js/darkmode.js' %}"></script>
</body>
</html>
