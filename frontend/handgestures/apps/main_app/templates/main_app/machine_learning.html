{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Machine Learning | Hand Motion Recognition Project</title>
  <link rel="stylesheet" href="{% static 'main_app/css/style.css' %}">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet">
  <link href="https://unpkg.com/aos@2.3.4/dist/aos.css" rel="stylesheet">
</head>

<body class="bg-light text-dark">
  {% include 'main_app/navbar.html' %}

  <main class="container wide-container py-5 mb-5">
    <!-- Hero Section -->
    <section class="mb-5">
      <div class="text-center text-white p-5 rounded-4 mx-3" style="background: linear-gradient(to right, #31b3fe, #7d7676);" data-aos="fade-down">
        <h1 class="fw-bold display-5 mb-4">Machine Learning Model</h1>
        <h2 class="lead text-light mb-0">Explore the algorithms, data processing, and training techniques behind our real-time motion recognition system.</h2>
      </div>
    </section>

    <!-- ML Flow Prototype Section -->
    <section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
      <h2 class="fw-bold text-primary mb-4">ML Flow Prototype</h2>
     <figure class="text-center">
         <img src="{% static 'main_app/img/ml-prototype.png' %}" 
         alt="ML Flow Prototype Diagram" 
         class="img-fluid shadow rounded mx-auto mb-2" 
         style="max-width: 1000px;">
        <figcaption class="text-muted fw-semibold small">
         Source: <a href="https://doi.org/10.1007/s11227-023-05299-9" target="_blank">Flow Prototype</a>
      </figcaption>
  </figure>
      <p class="text-muted fs-5" style="text-align: justify;">
        This flow diagram illustrates the complete machine learning pipeline used in our seizure detection system. From raw data acquisition to classification, each stage is designed to ensure reliable real-time performance in distinguishing between seizure and non-seizure motion patterns.
      </p>
      <ul class="text-muted fs-5">
       <li ><strong>Data Collection:</strong> IMU sensors capture real-time acceleration and rotation data from the user.</li>
      <li><strong>Preprocessing:</strong> Raw data is cleaned, normalized, and segmented into time windows for analysis.</li>
    <li><strong>Feature Extraction:</strong> Statistical and frequency-based features are extracted to represent motion patterns.</li>
    <li><strong>ML Methods:</strong> Machine learning models (e.g., Random Forest, SVM) are trained on labeled datasets.</li>
    <li><strong>Classification:</strong> The system classifies each time window as either “Seizure” or “Non-Seizure.”</li>
    <li><strong>Performance Evaluation:</strong> Accuracy, precision, recall, and confusion matrix are used to assess system effectiveness.</li>
  </ul>
</section>
<!-- Data Collection section-->
<section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
  <h2 class="fw-bold text-primary mb-4">Data Collection</h2>
  <div class="row mb-3">
    <div class="col-md-6">
      <img src="{% static 'main_app/img/hardware3.jpeg' %}" alt="Internal Hardware View" class="img-fluid shadow rounded mx-auto d-block" style="max-height: 504px;">
    </div>
    <div class="col-md-6">
      <img src="{% static 'main_app/img/hardware_outside.jpg' %}" alt="External Interface View" class="img-fluid shadow rounded mx-auto d-block" style="max-height: 504px;">
    </div>
  </div>
  <p class="text-muted fs-5" style="text-align: justify;">
    The wearable device is housed in a custom-built cardboard enclosure and includes a set of sensors and electronics for motion and distance tracking. 
    On the outside, a tactile push-button allows user interaction, and a compact OLED screen is embedded to display live feedback or status updates.
    Inside the enclosure, the system integrates multiple components wired together, including:
  </p>
  <ul class="text-muted fs-5">
    <li><strong>Microcontroller Unit:</strong> A central microcontroller (such as Arduino Nano) handles data processing and sensor communication.</li>
    <li><strong>IMU Module:</strong> Includes a gyroscope and accelerometer to track x, y, and z rotational and acceleration movements.</li>
    <li><strong>Ultrasonic Distance Sensors:</strong> Positioned to the sides to detect left and right distance from nearby surfaces for spatial awareness.</li>
    <li><strong>OLED Display:</strong> Mounted externally, it provides real-time display of sensor readings or status info.</li>
    <li><strong>Push Button:</strong> A user interface component to trigger data collection or mode changes.</li>
    <li><strong>Power Supply:</strong> A small Li-Po battery or USB connection powers the internal components.</li>
  </ul>
  <p class="text-muted fs-5" style="text-align: justify;">
    All collected data—such as motion vectors and proximity readings—is logged and written to a CSV file on the connected computer. 
    This setup is crucial for later analysis, such as training machine learning models for motion classification or seizure detection.
  </p>
</section>


    <!-- Data Pipeline Explanation Section -->
<section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
  <h2 class="fw-bold text-primary mb-4 text-center">Data Processing and Classification Pipeline</h2>
  
  <img src="{% static 'main_app/img/preprocessing.png' %}" alt="Preprocessing and Feature Extraction" class="img-fluid shadow rounded mx-auto d-block mb-4" style="max-width: 1000px;">

  <div class="text-muted fs-5" style="text-align: justify;">
    <p><strong>Raw Sensor Data:</strong> The system captures data from wearable sensors, including:
      <ul>
        <li><strong>Accelerometer (X, Y, Z)</strong> for detecting movement intensity and direction</li>
        <li><strong>Gyroscope (Roll, Pitch, Yaw)</strong> for tracking orientation and rotation</li>
        <li><strong>Ultrasound (Distance)</strong> for measuring proximity, aiding in fall detection</li>
      </ul>
    </p>

    <p><strong>Preprocessing:</strong> Before analysis, data undergoes:
      <ul>
        <li><strong>Filtering:</strong> Noise removal for cleaner signals</li>
        <li><strong>Segmentation:</strong> Dividing data into 5-second windows for temporal analysis</li>
        <li><strong>Normalization:</strong> Ensuring consistent data ranges across features</li>
      </ul>
    </p>

    <p><strong>Feature Extraction:</strong> Features are derived from sensor data using three domains:
      <ul>
        <li><strong>Time-Domain:</strong> Jerk (dAccel/dt), Zero-Crossing Rate, Signal Magnitude Area</li>
        <li><strong>Frequency-Domain:</strong> Power Spectral Density (PSD) peaks at 3–5Hz and Spectral Entropy</li>
        <li><strong>Time-Frequency:</strong> Wavelet Coefficients capturing motion over time and frequency</li>
      </ul>
    </p>

    <p><strong>Model Inference:</strong> An LSTM Neural Network (with 3 hidden layers and dropout regularization) processes the features. A threshold ensures only predictions with >90% confidence are considered.</p>

    <p><strong>Final Output:</strong> The model classifies the user’s movement into one of the following:
      <ul>
        <li><strong>Normal Movement</strong></li>
        <li><strong>Tonic-Clonic Seizure</strong> (identified via 3–5Hz signal frequency)</li>
        <li><strong>Myoclonic Seizure</strong> (shock-like movement pattern)</li>
        <li><strong>Postural Change / Fall</strong> (detected via wavelet-based pattern)</li>
      </ul>
    </p>
  </div>
</section>

<!--Machine Learning Methods-->
<section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
  <h2 class="fw-bold text-primary mb-4">Methods</h2>

  <p class="text-muted fs-5" style="text-align: justify;">
    Our system leverages a <strong>Semi-Supervised Learning</strong> approach to overcome the challenge of limited labeled sensor data. Initially, a small set of motion samples (such as Normal, Tonic-Clonic, Myoclonic, and Fall) were manually annotated. These samples were used to train a baseline model.
  </p>

  <p class="text-muted fs-5" style="text-align: justify;">
    This model was then used to assign <strong>pseudo-labels</strong> to a larger pool of unlabeled motion data. After selective verification, this expanded dataset improved model robustness while reducing manual labeling effort.
  </p>

  <p class="text-muted fs-5" style="text-align: justify;">
    The core architecture used in our project is a <strong>Long Short-Term Memory (LSTM)</strong> neural network, specifically designed to recognize patterns in time-series motion data.
  </p>

  <ul class="text-muted fs-5">
    <li><strong>Semi-Supervised Labeling:</strong> Combines a small amount of labeled data with large unlabeled data through pseudo-labeling.</li>
    <li><strong>LSTM Network:</strong> 3-layer recurrent model with dropout regularization for temporal pattern detection.</li>
    <li><strong>Sliding Window Input:</strong> Motion data is segmented into 5-second time windows for sequential analysis.</li>
    <li><strong>Confidence Filtering:</strong> Only predictions with >90% confidence are used to reinforce training or trigger alerts.</li>
  </ul>

  <p class="text-muted fs-5" style="text-align: justify;">
    This method balances labeling efficiency and model accuracy, enabling real-time classification of complex motion patterns such as seizure events and posture-related incidents.
  </p>
</section>


    <!-- Classification Section -->
    <section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
      <h2 class="fw-bold text-primary mb-4">Classification</h2>
      <img src="{% static 'main_app/img/classifications.png' %}" alt="Classifications" class="img-fluid shadow rounded mx-auto d-block mb-3" style="max-width: 1000px;">
    
      <p class="text-muted fs-5" style="text-align: justify;">
        Our seizure detection system classifies motion into four categories based on sensor input. These categories help distinguish between everyday movement and potentially dangerous seizure activity.
      </p>
    
      <ul class="text-muted fs-5">
        <li>
          <strong>Normal (No Seizure):</strong>  
          This classification represents typical daily movements such as walking, writing, eating, or adjusting posture. It indicates a stable condition with no irregular muscle contractions, no involuntary jerking, and consistent orientation. Sensor readings show smooth acceleration and rotation patterns within normal thresholds.
        </li>
        <li>
          <strong>Tremor (Hand Seizure):</strong>  
          Characterized by rhythmic shaking or trembling, usually in the hands. This type of seizure often results in rapid, repetitive motion patterns detected in the accelerometer data. The system identifies these patterns as fine oscillations with high frequency but low amplitude.
        </li>
        <li>
          <strong>Tonic (Arm Stiffening Seizure):</strong>  
          Marked by sudden and sustained muscle contractions, typically causing the arms to become rigid. The sensors detect a sudden increase in resistance to motion, minimal limb movement, and consistent stiff posturing. These are recognized through near-zero dynamic acceleration despite continuous muscle tension.
        </li>
        <li>
          <strong>Postural (Posture Change Seizure):</strong>  
          Involves sudden changes in body position or loss of balance, often leading to falls or abrupt leaning. The system identifies this through rapid shifts in acceleration and orientation, especially when combined with a drop in height or sideways tilting, suggesting instability or collapse.
        </li>
      </ul>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        These classifications allow the system to provide real-time alerts, support caregivers, and improve safety for individuals with epilepsy by monitoring movement patterns continuously and non-invasively.
      </p>
    </section>

    <!-- Performance Evaluation Section -->
    <section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
      <h2 class="fw-bold text-primary mb-4">Performance Evaluation</h2>
      <img src="{% static 'main_app/img/prediction_accuracy.png' %}" alt="Prediction Accuracy" class="img-fluid shadow rounded mx-auto d-block mb-3" style="max-width: 1000px;">
    
      <p class="text-muted fs-5" style="text-align: justify;">
        Our trained model achieved a high prediction accuracy of <strong>98.76%</strong>, calculated by comparing predicted labels with ground-truth annotations.
        This metric reflects the percentage of correct classifications across all categories.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        In addition to accuracy, further evaluation metrics such as <strong>precision, recall,</strong> and <strong>F1-score</strong> are calculated for each class 
        (Normal, Tremor, Tonic, Postural) to assess classification performance more comprehensively.
      </p>
    
      <p class="text-muted fs-5" style="text-align: justify;">
        Evaluation is based on a labeled dataset of <em>10000</em> samples, using a <em>87.65%</em> train/test split. 
        The model architecture is based on <em>4</em> types, optimized for real-time classification with wearable motion data.
      </p>
    </section>
    
    <!--
    TODO: decide if we want to include this section (commented out on purpose as of now; 5-12-2025)
    ML Pipeline Diagram Section 

    <section class="card shadow-lg p-4 mb-5 rounded" data-aos="fade-up">
      <h2 class="fw-bold text-primary mb-4">ML Pipeline Diagram</h2>
      <img src="{% static 'main_app/img/ml_pipeline.png' %}" alt="Machine Learning Pipeline Diagram" class="img-fluid mx-auto d-block mb-3" style="max-width: 900px;">

    </section>
        
    -->
    
  </main>

  {% include 'main_app/footer.html' %}

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
  <script src="https://unpkg.com/aos@2.3.4/dist/aos.js"></script>
  <script>
    AOS.init({ duration: 1000, once: true });
  </script>
  <script src="{% static 'main_app/js/darkmode.js' %}"></script>
</body>
</html>
